{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-01T16:38:07.573369Z",
     "start_time": "2024-08-01T16:38:04.290771Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cell_division.nets.transfer_learning import CNN\n",
    "from auxiliary.data.dataset_cell import CellDataset\n",
    "from auxiliary import values as v\n",
    "from auxiliary.utils.colors import bcolors as c\n",
    "\n",
    "# from focal_loss import SparseCategoricalFocalLoss\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from cell_division.nets.custom_layers import w_cel_loss, focal_loss\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# GPU config\n",
    "from auxiliary.utils.timer import LoadingBar\n",
    "from auxiliary.gpu.gpu_tf import (\n",
    "    increase_gpu_memory, \n",
    "    set_gpu_allocator, \n",
    "    clear_session\n",
    ")\n",
    "\n",
    "increase_gpu_memory()\n",
    "set_gpu_allocator()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T16:38:07.581265Z",
     "start_time": "2024-08-01T16:38:07.576134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_dir = v.data_path + 'CellDivision/images/'\n",
    "label_train_dir = v.data_path + 'CellDivision/train.csv'\n",
    "label_test_dir = v.data_path + 'CellDivision/test.csv'\n",
    "label_val_dir = v.data_path + 'CellDivision/val.csv'\n",
    "\n",
    "INPUT_SHAPE = (50, 50, 3)\n",
    "BATCH_SIZE = 64"
   ],
   "id": "72671286f4e09b93",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dataset (Generators)\n",
    "\n",
    "Generatos do not load directly the images into memory, but they load the images on the fly. This is useful when the dataset is too large to fit into memory."
   ],
   "id": "6a22a8984306870e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T16:38:07.614331Z",
     "start_time": "2024-08-01T16:38:07.582912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_generator = CellDataset(\n",
    "    img_dir, \n",
    "    label_train_dir, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    resize=INPUT_SHAPE[:2]\n",
    ")\n",
    "\n",
    "val_generator = CellDataset(\n",
    "    img_dir, \n",
    "    label_val_dir, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    resize=INPUT_SHAPE[:2]\n",
    ")"
   ],
   "id": "a54e19ea8c8c2bc9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transfer Learning ",
   "id": "bd19c08a9ae85271"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T16:38:07.621156Z",
     "start_time": "2024-08-01T16:38:07.616133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_models = {\n",
    "    'DenseNet121': tf.keras.applications.DenseNet121,\n",
    "    'EfficientNetV2L': tf.keras.applications.EfficientNetV2L,\n",
    "    'EfficientNetV2M': tf.keras.applications.EfficientNetV2M,\n",
    "    'VGG16': tf.keras.applications.VGG16,\n",
    "    'ResNet50': tf.keras.applications.ResNet50,\n",
    "    'InceptionV3': tf.keras.applications.InceptionV3,\n",
    "    'MobileNetV2': tf.keras.applications.MobileNetV2,\n",
    "    'NASNetMobile': tf.keras.applications.NASNetMobile,\n",
    "}\n"
   ],
   "id": "199d42b8fb6d8443",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T16:38:07.628149Z",
     "start_time": "2024-08-01T16:38:07.623791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'base_model': list(base_models.keys()),\n",
    "    'lr': [1e-3, 1e-2],\n",
    "    'fine_tune': [True, False],\n",
    "    'loss': [focal_loss(), w_cel_loss()],\n",
    "    'top': ['CAM', 'Standard'],\n",
    "    # 'class_weight': [None, 'balanced']\n",
    "}"
   ],
   "id": "49cc59a243f56557",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T08:50:21.459743Z",
     "start_time": "2024-08-02T07:35:00.089902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorboard.errors import InvalidArgumentError\n",
    "\n",
    "bar = LoadingBar(\n",
    "    len(param_grid['base_model']) * len(param_grid['lr']) * len(param_grid['fine_tune']) * len(param_grid['loss']) * len(param_grid['top'])\n",
    ")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for base_model in param_grid['base_model']:\n",
    "    for lr in param_grid['lr']:\n",
    "        for fine_tune in param_grid['fine_tune']:\n",
    "            for loss in param_grid['loss']:\n",
    "                for top in param_grid['top']:\n",
    "                    print(f'{c.OKGREEN}Model: {base_model} - LR: {lr} - Fine Tune: {fine_tune} - Loss: {loss} - Top: {top}{c.ENDC}')\n",
    "                    \n",
    "                    try:\n",
    "                        model = CNN(\n",
    "                            base=base_models[base_model],\n",
    "                            n_classes=3,\n",
    "                            input_shape=INPUT_SHAPE,\n",
    "                            fine_tune=fine_tune\n",
    "                        )\n",
    "                        model.build_top(activation='softmax', b_type=top)\n",
    "                        model.compile(lr=lr, loss=loss)\n",
    "                        model.fit(\n",
    "                            train_generator,\n",
    "                            val_generator,\n",
    "                            epochs=100,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            save=False,\n",
    "                            verbose=1\n",
    "                        )\n",
    "    \n",
    "                        results[(base_model, lr, fine_tune, loss, top)] = model.model.history\n",
    "                    except Exception as e:\n",
    "                        print(f'{c.FAIL}Error: {e}{c.ENDC}')\n",
    "                        results[(base_model, lr, fine_tune, loss, top)] = None\n",
    "\n",
    "                    clear_session()\n",
    "                    bar.update()\n",
    "\n",
    "bar.end()"
   ],
   "id": "52c645a63e00890e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 34s 2s/step - loss: 0.1696 - auc: 0.7088 - val_loss: 0.4569 - val_auc: 0.6992 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0501 - auc: 0.8416 - val_loss: 2.3818 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0393 - auc: 0.8872 - val_loss: 2.3034 - val_auc: 0.6617 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0274 - auc: 0.9512 - val_loss: 1.9266 - val_auc: 0.6916 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0142 - auc: 0.9897\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0142 - auc: 0.9897 - val_loss: 1.6552 - val_auc: 0.6899 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0051 - auc: 0.9993 - val_loss: 1.1151 - val_auc: 0.7126 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0030 - auc: 1.0000 - val_loss: 0.5573 - val_auc: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0018 - auc: 1.0000\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0018 - auc: 1.0000 - val_loss: 0.1978 - val_auc: 0.7676 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0014 - auc: 1.0000 - val_loss: 0.0841 - val_auc: 0.7952 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0013 - auc: 1.0000 - val_loss: 0.0605 - val_auc: 0.7670 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0013 - auc: 1.0000\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0013 - auc: 1.0000 - val_loss: 0.0625 - val_auc: 0.7164 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0013 - auc: 1.0000 - val_loss: 0.0691 - val_auc: 0.6745 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0012 - auc: 1.0000 - val_loss: 0.0744 - val_auc: 0.6436 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0012 - auc: 1.0000Restoring model weights from the end of the best epoch: 9.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0012 - auc: 1.0000 - val_loss: 0.0777 - val_auc: 0.6285 - lr: 1.0000e-06\n",
      "Epoch 14: early stopping\n",
      "[                                                  ] 0.38%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 26s 1s/step - loss: 0.2755 - auc: 0.6611 - val_loss: 0.3260 - val_auc: 0.6547 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.2003 - auc: 0.7426 - val_loss: 1.3249 - val_auc: 0.5047 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.1386 - auc: 0.7744 - val_loss: 3.4171 - val_auc: 0.4292 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.1146 - auc: 0.8015 - val_loss: 0.9216 - val_auc: 0.4613 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.1242 - auc: 0.7723 - val_loss: 2.1475 - val_auc: 0.6656 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0758 - auc: 0.8070\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0758 - auc: 0.8070 - val_loss: 1.9406 - val_auc: 0.6609 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0599 - auc: 0.8563 - val_loss: 0.8104 - val_auc: 0.6757 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0621 - auc: 0.8635 - val_loss: 0.3380 - val_auc: 0.7103 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0467 - auc: 0.8912\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0467 - auc: 0.8912 - val_loss: 0.1304 - val_auc: 0.7259 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0402 - auc: 0.8932 - val_loss: 0.0636 - val_auc: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0555 - auc: 0.8892 - val_loss: 0.0598 - val_auc: 0.7532 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0373 - auc: 0.9096\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0373 - auc: 0.9096 - val_loss: 0.0580 - val_auc: 0.7556 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0424 - auc: 0.9023 - val_loss: 0.0556 - val_auc: 0.7643 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0405 - auc: 0.9066 - val_loss: 0.0545 - val_auc: 0.7685 - lr: 1.0000e-06\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0415 - auc: 0.9078\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0415 - auc: 0.9078 - val_loss: 0.0535 - val_auc: 0.7751 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0440 - auc: 0.9031 - val_loss: 0.0525 - val_auc: 0.7859 - lr: 1.0000e-07\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0391 - auc: 0.9087 - val_loss: 0.0517 - val_auc: 0.7935 - lr: 1.0000e-07\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0450 - auc: 0.9009\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0450 - auc: 0.9009 - val_loss: 0.0511 - val_auc: 0.8003 - lr: 1.0000e-07\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0375 - auc: 0.9149 - val_loss: 0.0505 - val_auc: 0.8064 - lr: 1.0000e-08\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0458 - auc: 0.8988 - val_loss: 0.0501 - val_auc: 0.8128 - lr: 1.0000e-08\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0400 - auc: 0.9055\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0400 - auc: 0.9055 - val_loss: 0.0499 - val_auc: 0.8164 - lr: 1.0000e-08\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0424 - auc: 0.9064 - val_loss: 0.0497 - val_auc: 0.8191 - lr: 1.0000e-09\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0376 - auc: 0.9053 - val_loss: 0.0495 - val_auc: 0.8202 - lr: 1.0000e-09\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0419 - auc: 0.9122\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0419 - auc: 0.9122 - val_loss: 0.0493 - val_auc: 0.8201 - lr: 1.0000e-09\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0430 - auc: 0.9114 - val_loss: 0.0493 - val_auc: 0.8195 - lr: 1.0000e-10\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0442 - auc: 0.9043 - val_loss: 0.0493 - val_auc: 0.8194 - lr: 1.0000e-10\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0398 - auc: 0.9057\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0398 - auc: 0.9057 - val_loss: 0.0493 - val_auc: 0.8189 - lr: 1.0000e-10\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0409 - auc: 0.8997Restoring model weights from the end of the best epoch: 23.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0409 - auc: 0.8997 - val_loss: 0.0493 - val_auc: 0.8196 - lr: 1.0000e-11\n",
      "Epoch 28: early stopping\n",
      "[                                                  ] 0.76%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: True - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x762a9229c790> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 25s 1s/step - loss: 1.0215 - auc: 0.6986 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-04\n",
      "Epoch 6: early stopping\n",
      "[                                                  ] 1.14%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: True - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x762a9229c790> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 25s 1s/step - loss: 1.0304 - auc: 0.7156 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9481 - auc: 0.8282 - val_loss: 1.2355 - val_auc: 0.3659 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9109 - auc: 0.8733 - val_loss: 1.1926 - val_auc: 0.3669 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9085 - auc: 0.8747 - val_loss: 1.0869 - val_auc: 0.6052 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9031 - auc: 0.8848\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9031 - auc: 0.8848 - val_loss: 1.0234 - val_auc: 0.7357 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8682 - auc: 0.9093 - val_loss: 1.0217 - val_auc: 0.7428 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8477 - auc: 0.9218 - val_loss: 1.0103 - val_auc: 0.7501 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8354 - auc: 0.9313\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8354 - auc: 0.9313 - val_loss: 1.0034 - val_auc: 0.7655 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8313 - auc: 0.9380 - val_loss: 1.0169 - val_auc: 0.7622 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8296 - auc: 0.9382 - val_loss: 1.0317 - val_auc: 0.7305 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8273 - auc: 0.9385\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8273 - auc: 0.9385 - val_loss: 1.0492 - val_auc: 0.6866 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.8276 - auc: 0.9391 - val_loss: 1.0541 - val_auc: 0.6503 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8269 - auc: 0.9396Restoring model weights from the end of the best epoch: 8.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8269 - auc: 0.9396 - val_loss: 1.0545 - val_auc: 0.6303 - lr: 1.0000e-06\n",
      "Epoch 13: early stopping\n",
      "[                                                  ] 1.52%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: False - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 20s 1s/step - loss: 0.1390 - auc: 0.6254 - val_loss: 0.0837 - val_auc: 0.6836 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0600 - auc: 0.7470 - val_loss: 0.0557 - val_auc: 0.7389 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0475 - auc: 0.8449 - val_loss: 0.0533 - val_auc: 0.7797 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0435 - auc: 0.8723\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0435 - auc: 0.8723 - val_loss: 0.0506 - val_auc: 0.8034 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0406 - auc: 0.9014 - val_loss: 0.0505 - val_auc: 0.8049 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0401 - auc: 0.9040 - val_loss: 0.0498 - val_auc: 0.8099 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0397 - auc: 0.9065\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0397 - auc: 0.9065 - val_loss: 0.0498 - val_auc: 0.8116 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0394 - auc: 0.9079 - val_loss: 0.0497 - val_auc: 0.8112 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0393 - auc: 0.9087 - val_loss: 0.0497 - val_auc: 0.8123 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0393 - auc: 0.9090\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0393 - auc: 0.9090 - val_loss: 0.0497 - val_auc: 0.8123 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0392 - auc: 0.9096 - val_loss: 0.0497 - val_auc: 0.8122 - lr: 1.0000e-06\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0392 - auc: 0.9096 - val_loss: 0.0497 - val_auc: 0.8122 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0392 - auc: 0.9097\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0392 - auc: 0.9097 - val_loss: 0.0496 - val_auc: 0.8123 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8125 - lr: 1.0000e-07\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8125 - lr: 1.0000e-07\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0392 - auc: 0.9098\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8125 - lr: 1.0000e-07\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8127 - lr: 1.0000e-08\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8127 - lr: 1.0000e-08\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0392 - auc: 0.9098\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8127 - lr: 1.0000e-08\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8127 - lr: 1.0000e-09\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8127 - lr: 1.0000e-09\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0392 - auc: 0.9098Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0392 - auc: 0.9098 - val_loss: 0.0496 - val_auc: 0.8127 - lr: 1.0000e-09\n",
      "Epoch 22: early stopping\n",
      "[                                                  ] 1.89%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: False - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 21s 1s/step - loss: 0.2807 - auc: 0.6399 - val_loss: 0.0766 - val_auc: 0.6371 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.1676 - auc: 0.7540 - val_loss: 0.0750 - val_auc: 0.7315 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.1102 - auc: 0.8530 - val_loss: 0.0923 - val_auc: 0.7792 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0786 - auc: 0.8847\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0786 - auc: 0.8847 - val_loss: 0.1130 - val_auc: 0.7531 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0661 - auc: 0.9134 - val_loss: 0.1039 - val_auc: 0.7685 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0646 - auc: 0.9173 - val_loss: 0.1000 - val_auc: 0.7864 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0598 - auc: 0.9262\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0598 - auc: 0.9262 - val_loss: 0.1009 - val_auc: 0.7962 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0530 - auc: 0.9332 - val_loss: 0.0959 - val_auc: 0.7999 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0537 - auc: 0.9300 - val_loss: 0.0915 - val_auc: 0.8032 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0497 - auc: 0.9356\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 12s 999ms/step - loss: 0.0497 - auc: 0.9356 - val_loss: 0.0878 - val_auc: 0.8064 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0476 - auc: 0.9384 - val_loss: 0.0847 - val_auc: 0.8097 - lr: 1.0000e-06\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0545 - auc: 0.9271 - val_loss: 0.0820 - val_auc: 0.8121 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0539 - auc: 0.9301\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0539 - auc: 0.9301 - val_loss: 0.0796 - val_auc: 0.8145 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0503 - auc: 0.9334 - val_loss: 0.0776 - val_auc: 0.8175 - lr: 1.0000e-07\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0471 - auc: 0.9371 - val_loss: 0.0759 - val_auc: 0.8203 - lr: 1.0000e-07\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0393 - auc: 0.9483\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0393 - auc: 0.9483 - val_loss: 0.0744 - val_auc: 0.8230 - lr: 1.0000e-07\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0502 - auc: 0.9321 - val_loss: 0.0732 - val_auc: 0.8249 - lr: 1.0000e-08\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0520 - auc: 0.9312 - val_loss: 0.0722 - val_auc: 0.8265 - lr: 1.0000e-08\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0507 - auc: 0.9371\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0507 - auc: 0.9371 - val_loss: 0.0713 - val_auc: 0.8278 - lr: 1.0000e-08\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0477 - auc: 0.9362 - val_loss: 0.0707 - val_auc: 0.8305 - lr: 1.0000e-09\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0569 - auc: 0.9238 - val_loss: 0.0702 - val_auc: 0.8319 - lr: 1.0000e-09\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0461 - auc: 0.9374\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0461 - auc: 0.9374 - val_loss: 0.0698 - val_auc: 0.8336 - lr: 1.0000e-09\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0506 - auc: 0.9315 - val_loss: 0.0695 - val_auc: 0.8349 - lr: 1.0000e-10\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0564 - auc: 0.9233 - val_loss: 0.0693 - val_auc: 0.8366 - lr: 1.0000e-10\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0485 - auc: 0.9359\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0485 - auc: 0.9359 - val_loss: 0.0692 - val_auc: 0.8376 - lr: 1.0000e-10\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0538 - auc: 0.9257 - val_loss: 0.0691 - val_auc: 0.8391 - lr: 1.0000e-11\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0566 - auc: 0.9291 - val_loss: 0.0691 - val_auc: 0.8400 - lr: 1.0000e-11\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0498 - auc: 0.9357\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0498 - auc: 0.9357 - val_loss: 0.0692 - val_auc: 0.8406 - lr: 1.0000e-11\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0519 - auc: 0.9284 - val_loss: 0.0693 - val_auc: 0.8416 - lr: 1.0000e-12\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0510 - auc: 0.9289 - val_loss: 0.0694 - val_auc: 0.8421 - lr: 1.0000e-12\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0487 - auc: 0.9338\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0487 - auc: 0.9338 - val_loss: 0.0695 - val_auc: 0.8430 - lr: 1.0000e-12\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0492 - auc: 0.9359 - val_loss: 0.0697 - val_auc: 0.8440 - lr: 1.0000e-13\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0607 - auc: 0.9247 - val_loss: 0.0699 - val_auc: 0.8448 - lr: 1.0000e-13\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0524 - auc: 0.9328\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0524 - auc: 0.9328 - val_loss: 0.0701 - val_auc: 0.8448 - lr: 1.0000e-13\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0579 - auc: 0.9294 - val_loss: 0.0703 - val_auc: 0.8456 - lr: 1.0000e-14\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0601 - auc: 0.9247 - val_loss: 0.0705 - val_auc: 0.8459 - lr: 1.0000e-14\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0570 - auc: 0.9226\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0570 - auc: 0.9226 - val_loss: 0.0707 - val_auc: 0.8461 - lr: 1.0000e-14\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0553 - auc: 0.9292 - val_loss: 0.0709 - val_auc: 0.8465 - lr: 1.0000e-15\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0518 - auc: 0.9358 - val_loss: 0.0711 - val_auc: 0.8472 - lr: 1.0000e-15\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0522 - auc: 0.9336\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0522 - auc: 0.9336 - val_loss: 0.0713 - val_auc: 0.8478 - lr: 1.0000e-15\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0618 - auc: 0.9197 - val_loss: 0.0715 - val_auc: 0.8478 - lr: 1.0000e-16\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0530 - auc: 0.9307 - val_loss: 0.0717 - val_auc: 0.8478 - lr: 1.0000e-16\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0460 - auc: 0.9424\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0460 - auc: 0.9424 - val_loss: 0.0719 - val_auc: 0.8482 - lr: 1.0000e-16\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0528 - auc: 0.9300 - val_loss: 0.0721 - val_auc: 0.8487 - lr: 1.0000e-17\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0490 - auc: 0.9323 - val_loss: 0.0722 - val_auc: 0.8487 - lr: 1.0000e-17\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0442 - auc: 0.9411\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0442 - auc: 0.9411 - val_loss: 0.0724 - val_auc: 0.8488 - lr: 1.0000e-17\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0613 - auc: 0.9171 - val_loss: 0.0725 - val_auc: 0.8489 - lr: 1.0000e-18\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0599 - auc: 0.9210 - val_loss: 0.0727 - val_auc: 0.8488 - lr: 1.0000e-18\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0494 - auc: 0.9332\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0494 - auc: 0.9332 - val_loss: 0.0728 - val_auc: 0.8489 - lr: 1.0000e-18\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0536 - auc: 0.9323 - val_loss: 0.0729 - val_auc: 0.8491 - lr: 1.0000e-19\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0490 - auc: 0.9330 - val_loss: 0.0730 - val_auc: 0.8491 - lr: 1.0000e-19\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0529 - auc: 0.9255\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0529 - auc: 0.9255 - val_loss: 0.0732 - val_auc: 0.8488 - lr: 1.0000e-19\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0547 - auc: 0.9278 - val_loss: 0.0733 - val_auc: 0.8490 - lr: 1.0000e-20\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0637 - auc: 0.9224 - val_loss: 0.0734 - val_auc: 0.8492 - lr: 1.0000e-20\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0500 - auc: 0.9323\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0500 - auc: 0.9323 - val_loss: 0.0735 - val_auc: 0.8490 - lr: 1.0000e-20\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0565 - auc: 0.9282 - val_loss: 0.0735 - val_auc: 0.8493 - lr: 1.0000e-21\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0518 - auc: 0.9323 - val_loss: 0.0736 - val_auc: 0.8492 - lr: 1.0000e-21\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0575 - auc: 0.9208\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0575 - auc: 0.9208 - val_loss: 0.0737 - val_auc: 0.8494 - lr: 1.0000e-21\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0526 - auc: 0.9317 - val_loss: 0.0738 - val_auc: 0.8495 - lr: 1.0000e-22\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0533 - auc: 0.9291 - val_loss: 0.0738 - val_auc: 0.8495 - lr: 1.0000e-22\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0530 - auc: 0.9359\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0530 - auc: 0.9359 - val_loss: 0.0739 - val_auc: 0.8497 - lr: 1.0000e-22\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0503 - auc: 0.9362 - val_loss: 0.0739 - val_auc: 0.8496 - lr: 1.0000e-23\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0489 - auc: 0.9351 - val_loss: 0.0740 - val_auc: 0.8496 - lr: 1.0000e-23\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0567 - auc: 0.9259\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0567 - auc: 0.9259 - val_loss: 0.0740 - val_auc: 0.8495 - lr: 1.0000e-23\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 17s 1s/step - loss: 0.0494 - auc: 0.9379 - val_loss: 0.0741 - val_auc: 0.8493 - lr: 1.0000e-24\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0473 - auc: 0.9355Restoring model weights from the end of the best epoch: 61.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0473 - auc: 0.9355 - val_loss: 0.0741 - val_auc: 0.8495 - lr: 1.0000e-24\n",
      "Epoch 66: early stopping\n",
      "[=                                                 ] 2.27%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: False - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x762a9229c790> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 20s 1s/step - loss: 1.0195 - auc: 0.6988 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-04\n",
      "Epoch 6: early stopping\n",
      "[=                                                 ] 2.65%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: False - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x762a9229c790> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 21s 1s/step - loss: 1.0631 - auc: 0.6635 - val_loss: 1.1528 - val_auc: 0.4634 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9867 - auc: 0.7815 - val_loss: 1.0954 - val_auc: 0.6007 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9566 - auc: 0.8318 - val_loss: 1.0497 - val_auc: 0.6963 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9274 - auc: 0.8754\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9274 - auc: 0.8754 - val_loss: 1.0274 - val_auc: 0.7580 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9009 - auc: 0.9066 - val_loss: 1.0217 - val_auc: 0.7721 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8929 - auc: 0.9136 - val_loss: 1.0203 - val_auc: 0.7786 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8892 - auc: 0.9173\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8892 - auc: 0.9173 - val_loss: 1.0162 - val_auc: 0.7853 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8838 - auc: 0.9171 - val_loss: 1.0138 - val_auc: 0.7888 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8830 - auc: 0.9229 - val_loss: 1.0116 - val_auc: 0.7919 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8815 - auc: 0.9245\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8815 - auc: 0.9245 - val_loss: 1.0094 - val_auc: 0.7959 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8860 - auc: 0.9206 - val_loss: 1.0073 - val_auc: 0.7983 - lr: 1.0000e-06\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8822 - auc: 0.9254 - val_loss: 1.0051 - val_auc: 0.8014 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8875 - auc: 0.9210\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8875 - auc: 0.9210 - val_loss: 1.0031 - val_auc: 0.8043 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8836 - auc: 0.9230 - val_loss: 1.0010 - val_auc: 0.8068 - lr: 1.0000e-07\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.8851 - auc: 0.9229 - val_loss: 0.9991 - val_auc: 0.8078 - lr: 1.0000e-07\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8870 - auc: 0.9184\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8870 - auc: 0.9184 - val_loss: 0.9971 - val_auc: 0.8097 - lr: 1.0000e-07\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8842 - auc: 0.9207 - val_loss: 0.9953 - val_auc: 0.8111 - lr: 1.0000e-08\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8888 - auc: 0.9172 - val_loss: 0.9936 - val_auc: 0.8130 - lr: 1.0000e-08\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8863 - auc: 0.9239\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.8863 - auc: 0.9239 - val_loss: 0.9920 - val_auc: 0.8132 - lr: 1.0000e-08\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8813 - auc: 0.9258 - val_loss: 0.9905 - val_auc: 0.8136 - lr: 1.0000e-09\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8813 - auc: 0.9249 - val_loss: 0.9891 - val_auc: 0.8152 - lr: 1.0000e-09\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8828 - auc: 0.9241\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8828 - auc: 0.9241 - val_loss: 0.9879 - val_auc: 0.8163 - lr: 1.0000e-09\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8892 - auc: 0.9211 - val_loss: 0.9867 - val_auc: 0.8172 - lr: 1.0000e-10\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8862 - auc: 0.9208 - val_loss: 0.9857 - val_auc: 0.8180 - lr: 1.0000e-10\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8841 - auc: 0.9224\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8841 - auc: 0.9224 - val_loss: 0.9848 - val_auc: 0.8193 - lr: 1.0000e-10\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8865 - auc: 0.9241 - val_loss: 0.9839 - val_auc: 0.8201 - lr: 1.0000e-11\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8871 - auc: 0.9192 - val_loss: 0.9832 - val_auc: 0.8209 - lr: 1.0000e-11\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8857 - auc: 0.9211\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8857 - auc: 0.9211 - val_loss: 0.9825 - val_auc: 0.8217 - lr: 1.0000e-11\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8838 - auc: 0.9240 - val_loss: 0.9819 - val_auc: 0.8213 - lr: 1.0000e-12\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8880 - auc: 0.9208 - val_loss: 0.9813 - val_auc: 0.8221 - lr: 1.0000e-12\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8854 - auc: 0.9206\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8854 - auc: 0.9206 - val_loss: 0.9808 - val_auc: 0.8231 - lr: 1.0000e-12\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8834 - auc: 0.9220 - val_loss: 0.9804 - val_auc: 0.8239 - lr: 1.0000e-13\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8857 - auc: 0.9175 - val_loss: 0.9800 - val_auc: 0.8237 - lr: 1.0000e-13\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8846 - auc: 0.9204\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8846 - auc: 0.9204 - val_loss: 0.9796 - val_auc: 0.8245 - lr: 1.0000e-13\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8847 - auc: 0.9190 - val_loss: 0.9793 - val_auc: 0.8248 - lr: 1.0000e-14\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8890 - auc: 0.9151 - val_loss: 0.9789 - val_auc: 0.8244 - lr: 1.0000e-14\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8819 - auc: 0.9224\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.8819 - auc: 0.9224 - val_loss: 0.9787 - val_auc: 0.8249 - lr: 1.0000e-14\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8826 - auc: 0.9219 - val_loss: 0.9784 - val_auc: 0.8256 - lr: 1.0000e-15\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8887 - auc: 0.9179 - val_loss: 0.9782 - val_auc: 0.8260 - lr: 1.0000e-15\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8835 - auc: 0.9195\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8835 - auc: 0.9195 - val_loss: 0.9780 - val_auc: 0.8265 - lr: 1.0000e-15\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8872 - auc: 0.9174 - val_loss: 0.9778 - val_auc: 0.8271 - lr: 1.0000e-16\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.8899 - auc: 0.9190 - val_loss: 0.9776 - val_auc: 0.8273 - lr: 1.0000e-16\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8842 - auc: 0.9220\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8842 - auc: 0.9220 - val_loss: 0.9775 - val_auc: 0.8280 - lr: 1.0000e-16\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8837 - auc: 0.9224 - val_loss: 0.9773 - val_auc: 0.8257 - lr: 1.0000e-17\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8852 - auc: 0.9220 - val_loss: 0.9772 - val_auc: 0.8258 - lr: 1.0000e-17\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8824 - auc: 0.9242\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8824 - auc: 0.9242 - val_loss: 0.9770 - val_auc: 0.8259 - lr: 1.0000e-17\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8813 - auc: 0.9262 - val_loss: 0.9769 - val_auc: 0.8263 - lr: 1.0000e-18\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8842 - auc: 0.9227Restoring model weights from the end of the best epoch: 43.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.8842 - auc: 0.9227 - val_loss: 0.9768 - val_auc: 0.8260 - lr: 1.0000e-18\n",
      "Epoch 48: early stopping\n",
      "[=                                                 ] 3.03%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 25s 1s/step - loss: 1.9659 - auc: 0.6969 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 2.1259 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 2.1259 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1260 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 2.1260 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 2.1259 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1259 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 2.1259 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[=                                                 ] 3.41%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 26s 1s/step - loss: 0.2722 - auc: 0.5957 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.1650 - auc: 0.7741 - val_loss: 4.1661 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0798 - auc: 0.7517 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0617 - auc: 0.8114 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0589 - auc: 0.8289\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0589 - auc: 0.8289 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0533 - auc: 0.8581Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0533 - auc: 0.8581 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[=                                                 ] 3.79%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: True - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x762a9229c790> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 26s 1s/step - loss: 1.0197 - auc: 0.6987 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[==                                                ] 4.17%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: True - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x762a9229c790> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 26s 1s/step - loss: 1.0442 - auc: 0.6779 - val_loss: 1.2233 - val_auc: 0.4273 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0227 - auc: 0.6846 - val_loss: 1.2233 - val_auc: 0.4273 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0301 - auc: 0.6747 - val_loss: 1.2233 - val_auc: 0.4273 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0293 - auc: 0.6751\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0293 - auc: 0.6751 - val_loss: 1.2233 - val_auc: 0.4273 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0277 - auc: 0.6773 - val_loss: 1.2233 - val_auc: 0.4273 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0277 - auc: 0.6773Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0277 - auc: 0.6773 - val_loss: 1.2233 - val_auc: 0.4273 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[==                                                ] 4.55%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: False - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 22s 1s/step - loss: 1.9573 - auc: 0.6997 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 2.1259 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 2.1259 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1259 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 2.1259 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 2.1259 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1260 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 2.1260 - auc: 0.7008 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[==                                                ] 4.92%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: False - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 21s 1s/step - loss: 0.5003 - auc: 0.6451 - val_loss: 0.6125 - val_auc: 0.6413 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.1681 - auc: 0.7790 - val_loss: 0.7356 - val_auc: 0.5296 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.1501 - auc: 0.7968 - val_loss: 0.2331 - val_auc: 0.6845 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.1178 - auc: 0.8177 - val_loss: 0.4127 - val_auc: 0.7375 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1001 - auc: 0.8322\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.1001 - auc: 0.8322 - val_loss: 0.2822 - val_auc: 0.6827 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0883 - auc: 0.8600 - val_loss: 0.1242 - val_auc: 0.8029 - lr: 1.0000e-03\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0661 - auc: 0.8853 - val_loss: 0.1044 - val_auc: 0.8122 - lr: 1.0000e-03\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0522 - auc: 0.9114\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0522 - auc: 0.9114 - val_loss: 0.1066 - val_auc: 0.8215 - lr: 1.0000e-03\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0607 - auc: 0.9052 - val_loss: 0.1005 - val_auc: 0.8254 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0463 - auc: 0.9190 - val_loss: 0.0957 - val_auc: 0.8283 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0533 - auc: 0.9087\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0533 - auc: 0.9087 - val_loss: 0.0913 - val_auc: 0.8303 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0473 - auc: 0.9207 - val_loss: 0.0878 - val_auc: 0.8311 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0507 - auc: 0.9166 - val_loss: 0.0850 - val_auc: 0.8322 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0571 - auc: 0.9046\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0571 - auc: 0.9046 - val_loss: 0.0828 - val_auc: 0.8334 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0538 - auc: 0.9060 - val_loss: 0.0809 - val_auc: 0.8342 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0458 - auc: 0.9151 - val_loss: 0.0793 - val_auc: 0.8347 - lr: 1.0000e-06\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0565 - auc: 0.9113\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0565 - auc: 0.9113 - val_loss: 0.0780 - val_auc: 0.8357 - lr: 1.0000e-06\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0528 - auc: 0.9107 - val_loss: 0.0769 - val_auc: 0.8360 - lr: 1.0000e-07\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0482 - auc: 0.9215 - val_loss: 0.0759 - val_auc: 0.8365 - lr: 1.0000e-07\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0471 - auc: 0.9199\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0471 - auc: 0.9199 - val_loss: 0.0752 - val_auc: 0.8375 - lr: 1.0000e-07\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0525 - auc: 0.9081 - val_loss: 0.0745 - val_auc: 0.8377 - lr: 1.0000e-08\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0529 - auc: 0.9107 - val_loss: 0.0740 - val_auc: 0.8382 - lr: 1.0000e-08\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0526 - auc: 0.9120\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0526 - auc: 0.9120 - val_loss: 0.0735 - val_auc: 0.8383 - lr: 1.0000e-08\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0501 - auc: 0.9150 - val_loss: 0.0731 - val_auc: 0.8387 - lr: 1.0000e-09\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0468 - auc: 0.9195 - val_loss: 0.0728 - val_auc: 0.8389 - lr: 1.0000e-09\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0474 - auc: 0.9191\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0474 - auc: 0.9191 - val_loss: 0.0725 - val_auc: 0.8392 - lr: 1.0000e-09\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0529 - auc: 0.9080 - val_loss: 0.0722 - val_auc: 0.8393 - lr: 1.0000e-10\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0536 - auc: 0.9097 - val_loss: 0.0720 - val_auc: 0.8396 - lr: 1.0000e-10\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0497 - auc: 0.9123\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0497 - auc: 0.9123 - val_loss: 0.0718 - val_auc: 0.8396 - lr: 1.0000e-10\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0513 - auc: 0.9128 - val_loss: 0.0717 - val_auc: 0.8395 - lr: 1.0000e-11\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0487 - auc: 0.9153 - val_loss: 0.0715 - val_auc: 0.8400 - lr: 1.0000e-11\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0542 - auc: 0.9105\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0542 - auc: 0.9105 - val_loss: 0.0714 - val_auc: 0.8401 - lr: 1.0000e-11\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0522 - auc: 0.9079 - val_loss: 0.0714 - val_auc: 0.8403 - lr: 1.0000e-12\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0551 - auc: 0.9042 - val_loss: 0.0713 - val_auc: 0.8403 - lr: 1.0000e-12\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0503 - auc: 0.9134\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0503 - auc: 0.9134 - val_loss: 0.0712 - val_auc: 0.8403 - lr: 1.0000e-12\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0488 - auc: 0.9149 - val_loss: 0.0711 - val_auc: 0.8403 - lr: 1.0000e-13\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0552 - auc: 0.9049 - val_loss: 0.0711 - val_auc: 0.8403 - lr: 1.0000e-13\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0570 - auc: 0.9032\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0570 - auc: 0.9032 - val_loss: 0.0710 - val_auc: 0.8404 - lr: 1.0000e-13\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0534 - auc: 0.9077 - val_loss: 0.0710 - val_auc: 0.8404 - lr: 1.0000e-14\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0549 - auc: 0.9110 - val_loss: 0.0710 - val_auc: 0.8405 - lr: 1.0000e-14\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0485 - auc: 0.9161\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0485 - auc: 0.9161 - val_loss: 0.0709 - val_auc: 0.8407 - lr: 1.0000e-14\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 17s 1s/step - loss: 0.0509 - auc: 0.9108 - val_loss: 0.0709 - val_auc: 0.8407 - lr: 1.0000e-15\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0538 - auc: 0.9063 - val_loss: 0.0709 - val_auc: 0.8407 - lr: 1.0000e-15\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0528 - auc: 0.9120\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0528 - auc: 0.9120 - val_loss: 0.0709 - val_auc: 0.8405 - lr: 1.0000e-15\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0508 - auc: 0.9135 - val_loss: 0.0709 - val_auc: 0.8405 - lr: 1.0000e-16\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0565 - auc: 0.9070 - val_loss: 0.0709 - val_auc: 0.8406 - lr: 1.0000e-16\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0514 - auc: 0.9098Restoring model weights from the end of the best epoch: 42.\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0514 - auc: 0.9098 - val_loss: 0.0709 - val_auc: 0.8406 - lr: 1.0000e-16\n",
      "Epoch 47: early stopping\n",
      "[==                                                ] 5.30%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: False - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x762a9229c790> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 20s 1s/step - loss: 1.0103 - auc: 0.7112 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[==                                                ] 5.68%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: False - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x762a9229c790> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 21s 1s/step - loss: 1.0788 - auc: 0.6363 - val_loss: 1.1893 - val_auc: 0.4659 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 1.0113 - auc: 0.7268 - val_loss: 1.1164 - val_auc: 0.5677 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9772 - auc: 0.7673 - val_loss: 1.0343 - val_auc: 0.6847 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9658 - auc: 0.7717\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9658 - auc: 0.7717 - val_loss: 1.0610 - val_auc: 0.6484 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9601 - auc: 0.7871 - val_loss: 1.0324 - val_auc: 0.6875 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9587 - auc: 0.7926 - val_loss: 1.0178 - val_auc: 0.7160 - lr: 1.0000e-03\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9440 - auc: 0.8021\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9440 - auc: 0.8021 - val_loss: 1.0110 - val_auc: 0.7292 - lr: 1.0000e-03\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9438 - auc: 0.8048 - val_loss: 1.0094 - val_auc: 0.7288 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.9464 - auc: 0.8035 - val_loss: 1.0079 - val_auc: 0.7365 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9449 - auc: 0.8074\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.9449 - auc: 0.8074 - val_loss: 1.0061 - val_auc: 0.7363 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9475 - auc: 0.8016 - val_loss: 1.0020 - val_auc: 0.7411 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9440 - auc: 0.8046 - val_loss: 0.9981 - val_auc: 0.7429 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9441 - auc: 0.8081\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.9441 - auc: 0.8081 - val_loss: 0.9942 - val_auc: 0.7476 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.9420 - auc: 0.8059 - val_loss: 0.9905 - val_auc: 0.7521 - lr: 1.0000e-06\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9441 - auc: 0.8044 - val_loss: 0.9874 - val_auc: 0.7528 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9448 - auc: 0.8054\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9448 - auc: 0.8054 - val_loss: 0.9848 - val_auc: 0.7572 - lr: 1.0000e-06\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9423 - auc: 0.8029 - val_loss: 0.9828 - val_auc: 0.7628 - lr: 1.0000e-07\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9423 - auc: 0.8026 - val_loss: 0.9811 - val_auc: 0.7611 - lr: 1.0000e-07\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9457 - auc: 0.8053\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9457 - auc: 0.8053 - val_loss: 0.9798 - val_auc: 0.7611 - lr: 1.0000e-07\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9366 - auc: 0.8091 - val_loss: 0.9788 - val_auc: 0.7634 - lr: 1.0000e-08\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9458 - auc: 0.8063 - val_loss: 0.9780 - val_auc: 0.7629 - lr: 1.0000e-08\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9472 - auc: 0.8006\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.9472 - auc: 0.8006 - val_loss: 0.9773 - val_auc: 0.7649 - lr: 1.0000e-08\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.9439 - auc: 0.8073 - val_loss: 0.9768 - val_auc: 0.7639 - lr: 1.0000e-09\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9450 - auc: 0.8048 - val_loss: 0.9763 - val_auc: 0.7672 - lr: 1.0000e-09\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9417 - auc: 0.8032\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9417 - auc: 0.8032 - val_loss: 0.9759 - val_auc: 0.7669 - lr: 1.0000e-09\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9402 - auc: 0.8080 - val_loss: 0.9756 - val_auc: 0.7663 - lr: 1.0000e-10\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9465 - auc: 0.8100 - val_loss: 0.9753 - val_auc: 0.7696 - lr: 1.0000e-10\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9491 - auc: 0.8015\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9491 - auc: 0.8015 - val_loss: 0.9750 - val_auc: 0.7696 - lr: 1.0000e-10\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9445 - auc: 0.8126 - val_loss: 0.9748 - val_auc: 0.7701 - lr: 1.0000e-11\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9413 - auc: 0.8078 - val_loss: 0.9746 - val_auc: 0.7701 - lr: 1.0000e-11\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9432 - auc: 0.8047\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9432 - auc: 0.8047 - val_loss: 0.9744 - val_auc: 0.7702 - lr: 1.0000e-11\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.9439 - auc: 0.7993 - val_loss: 0.9743 - val_auc: 0.7701 - lr: 1.0000e-12\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.9456 - auc: 0.7992 - val_loss: 0.9741 - val_auc: 0.7700 - lr: 1.0000e-12\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9426 - auc: 0.8046\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9426 - auc: 0.8046 - val_loss: 0.9740 - val_auc: 0.7693 - lr: 1.0000e-12\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9430 - auc: 0.8054 - val_loss: 0.9739 - val_auc: 0.7693 - lr: 1.0000e-13\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9382 - auc: 0.8075Restoring model weights from the end of the best epoch: 31.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9382 - auc: 0.8075 - val_loss: 0.9738 - val_auc: 0.7694 - lr: 1.0000e-13\n",
      "Epoch 36: early stopping\n",
      "[===                                               ] 6.06%\u001B[92mModel: DenseNet121 - LR: 0.1 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x762a9229c280> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 3.8543 - auc: 0.4105"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_46368/11548542.py\", line 22, in <module>\n      model.fit(\n    File \"/home/imarcoss/ht_morphogenesis/cell_division/nets/transfer_learning.py\", line 93, in fit\n      )\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1420, in fit\n      val_logs = self.evaluate(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1716, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1525, in test_function\n      return step_function(self, iterator)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1514, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1507, in run_step\n      outputs = model.test_step(data)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1474, in test_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 957, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 459, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/metrics.py\", line 178, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/metrics.py\", line 2347, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.compat.v1.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_46368/11548542.py\", line 22, in <module>\n      model.fit(\n    File \"/home/imarcoss/ht_morphogenesis/cell_division/nets/transfer_learning.py\", line 93, in fit\n      )\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1420, in fit\n      val_logs = self.evaluate(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1716, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1525, in test_function\n      return step_function(self, iterator)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1514, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1507, in run_step\n      outputs = model.test_step(data)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1474, in test_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 957, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 459, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/metrics.py\", line 178, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/metrics.py\", line 2347, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.compat.v1.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model/prediction_layer/Softmax:0) = ] [[nan nan nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_33]]\n  (1) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model/prediction_layer/Softmax:0) = ] [[nan nan nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_2645114]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39mbuild_top(activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m'\u001B[39m, b_type\u001B[38;5;241m=\u001B[39mtop)\n\u001B[1;32m     21\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(lr\u001B[38;5;241m=\u001B[39mlr, loss\u001B[38;5;241m=\u001B[39mloss)\n\u001B[0;32m---> 22\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m     29\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m results[(base_model, lr, fine_tune, loss, top)] \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mhistory\n\u001B[1;32m     33\u001B[0m clear_session()\n",
      "File \u001B[0;32m~/ht_morphogenesis/cell_division/nets/transfer_learning.py:93\u001B[0m, in \u001B[0;36mfit\u001B[0;34m(self, train_gen, val_gen, epochs, batch_size, save, verbose)\u001B[0m\n\u001B[1;32m     82\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     83\u001B[0m     EarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_auc\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[1;32m     84\u001B[0m     ReduceLROnPlateau(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_auc\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     85\u001B[0m ]\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m save:\n\u001B[1;32m     88\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m     89\u001B[0m         ModelCheckpoint(\n\u001B[1;32m     90\u001B[0m             v\u001B[38;5;241m.\u001B[39mdata_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/cellular_division_models/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.h5\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     91\u001B[0m             save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, save_weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     92\u001B[0m             monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_auc\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 93\u001B[0m         )\n\u001B[1;32m     94\u001B[0m     )\n\u001B[1;32m     96\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mfit(\n\u001B[1;32m     97\u001B[0m     train_gen,\n\u001B[1;32m     98\u001B[0m     validation_data\u001B[38;5;241m=\u001B[39mval_gen,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    102\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose\n\u001B[1;32m    103\u001B[0m )\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;66;03m# self.model.summary()\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Graph execution error:\n\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_46368/11548542.py\", line 22, in <module>\n      model.fit(\n    File \"/home/imarcoss/ht_morphogenesis/cell_division/nets/transfer_learning.py\", line 93, in fit\n      )\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1420, in fit\n      val_logs = self.evaluate(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1716, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1525, in test_function\n      return step_function(self, iterator)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1514, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1507, in run_step\n      outputs = model.test_step(data)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1474, in test_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 957, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 459, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/metrics.py\", line 178, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/metrics.py\", line 2347, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.compat.v1.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_46368/11548542.py\", line 22, in <module>\n      model.fit(\n    File \"/home/imarcoss/ht_morphogenesis/cell_division/nets/transfer_learning.py\", line 93, in fit\n      )\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1420, in fit\n      val_logs = self.evaluate(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1716, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1525, in test_function\n      return step_function(self, iterator)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1514, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1507, in run_step\n      outputs = model.test_step(data)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1474, in test_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/training.py\", line 957, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 459, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/metrics.py\", line 178, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/metrics.py\", line 2347, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"/home/imarcoss/mambaforge/envs/py310ml/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.compat.v1.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model/prediction_layer/Softmax:0) = ] [[nan nan nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_33]]\n  (1) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model/prediction_layer/Softmax:0) = ] [[nan nan nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_2645114]"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "print(json.dumps(results, indent=4))\n",
    "# Save\n",
    "with open('../cell_division/results/grid_search_cnn.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ],
   "id": "87db1c7a4b035978",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
