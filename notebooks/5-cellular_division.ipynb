{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-02T14:40:46.510521Z",
     "start_time": "2024-08-02T14:40:43.252069Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cell_division.nets.transfer_learning import CNN\n",
    "from auxiliary.data.dataset_cell import CellDataset\n",
    "from auxiliary import values as v\n",
    "from auxiliary.utils.colors import bcolors as c\n",
    "\n",
    "# from focal_loss import SparseCategoricalFocalLoss\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from cell_division.nets.custom_layers import w_cel_loss, focal_loss\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# GPU config\n",
    "from auxiliary.utils.timer import LoadingBar\n",
    "from auxiliary.gpu.gpu_tf import (\n",
    "    increase_gpu_memory, \n",
    "    set_gpu_allocator, \n",
    "    clear_session\n",
    ")\n",
    "\n",
    "increase_gpu_memory()\n",
    "set_gpu_allocator()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T14:40:46.518146Z",
     "start_time": "2024-08-02T14:40:46.513077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_dir = v.data_path + 'CellDivision/images/'\n",
    "label_train_dir = v.data_path + 'CellDivision/train.csv'\n",
    "label_test_dir = v.data_path + 'CellDivision/test.csv'\n",
    "label_val_dir = v.data_path + 'CellDivision/val.csv'\n",
    "\n",
    "INPUT_SHAPE = (50, 50, 3)\n",
    "BATCH_SIZE = 64"
   ],
   "id": "72671286f4e09b93",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dataset (Generators)\n",
    "\n",
    "Generatos do not load directly the images into memory, but they load the images on the fly. This is useful when the dataset is too large to fit into memory."
   ],
   "id": "6a22a8984306870e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T14:40:46.641187Z",
     "start_time": "2024-08-02T14:40:46.519499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_generator = CellDataset(\n",
    "    img_dir, \n",
    "    label_train_dir, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    resize=INPUT_SHAPE[:2]\n",
    ")\n",
    "\n",
    "val_generator = CellDataset(\n",
    "    img_dir, \n",
    "    label_val_dir, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    resize=INPUT_SHAPE[:2]\n",
    ")"
   ],
   "id": "a54e19ea8c8c2bc9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transfer Learning ",
   "id": "bd19c08a9ae85271"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T14:40:46.646825Z",
     "start_time": "2024-08-02T14:40:46.642380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_models = {\n",
    "    'DenseNet121': tf.keras.applications.DenseNet121,\n",
    "    'EfficientNetV2L': tf.keras.applications.EfficientNetV2L,\n",
    "    'EfficientNetV2M': tf.keras.applications.EfficientNetV2M,\n",
    "    'VGG16': tf.keras.applications.VGG16,\n",
    "    'ResNet50': tf.keras.applications.ResNet50,\n",
    "    'InceptionV3': tf.keras.applications.InceptionV3,\n",
    "    'MobileNetV2': tf.keras.applications.MobileNetV2,\n",
    "    'NASNetMobile': tf.keras.applications.NASNetMobile,\n",
    "}\n"
   ],
   "id": "199d42b8fb6d8443",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T14:40:46.654758Z",
     "start_time": "2024-08-02T14:40:46.650081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'base_model': list(base_models.keys()),\n",
    "    'lr': [1e-3, 1e-2],\n",
    "    'fine_tune': [True, False],\n",
    "    'loss': [focal_loss(), w_cel_loss()],\n",
    "    'top': ['CAM', 'Standard'],\n",
    "    # 'class_weight': [None, 'balanced']\n",
    "}"
   ],
   "id": "49cc59a243f56557",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T15:22:47.615363Z",
     "start_time": "2024-08-02T14:40:46.656300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorboard.errors import InvalidArgumentError\n",
    "\n",
    "bar = LoadingBar(\n",
    "    len(param_grid['base_model']) * len(param_grid['lr']) * len(param_grid['fine_tune']) * len(param_grid['loss']) * len(param_grid['top'])\n",
    ")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for base_model in param_grid['base_model']:\n",
    "    for lr in param_grid['lr']:\n",
    "        for fine_tune in param_grid['fine_tune']:\n",
    "            for loss in param_grid['loss']:\n",
    "                for top in param_grid['top']:\n",
    "                    print(f'{c.OKGREEN}Model: {base_model} - LR: {lr} - Fine Tune: {fine_tune} - Loss: {loss} - Top: {top}{c.ENDC}')\n",
    "                    \n",
    "                    try:\n",
    "                        model = CNN(\n",
    "                            base=base_models[base_model],\n",
    "                            n_classes=3,\n",
    "                            input_shape=INPUT_SHAPE,\n",
    "                            fine_tune=fine_tune\n",
    "                        )\n",
    "                        model.build_top(activation='softmax', b_type=top)\n",
    "                        model.compile(lr=lr, loss=loss)\n",
    "                        model.fit(\n",
    "                            train_generator,\n",
    "                            val_generator,\n",
    "                            epochs=100,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            save=False,\n",
    "                            verbose=1\n",
    "                        )\n",
    "    \n",
    "                        results[\n",
    "                            str((base_model, lr, fine_tune, loss.__name__, top))\n",
    "                        ] = model.model.history.history\n",
    "                    except Exception as e:\n",
    "                        print(f'{c.FAIL}Error: {e}{c.ENDC}')\n",
    "                        results[(base_model, lr, fine_tune, loss, top)] = None\n",
    "\n",
    "                    clear_session()\n",
    "                    bar.update()\n",
    "                    \n",
    "    with open(f'../cell_division/results/{base_model}.json', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    results = {}\n",
    "\n",
    "bar.end()"
   ],
   "id": "52c645a63e00890e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x700d0b328790> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 35s 2s/step - loss: 0.3170 - auc: 0.6351 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 918ms/step - loss: 0.0633 - auc: 0.7134 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 893ms/step - loss: 0.0679 - auc: 0.8226 - val_loss: 4.0692 - val_auc: 0.4273 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 11s 916ms/step - loss: 0.0461 - auc: 0.8658 - val_loss: 4.0526 - val_auc: 0.4265 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 967ms/step - loss: 0.0362 - auc: 0.9068 - val_loss: 2.3432 - val_auc: 0.6617 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0286 - auc: 0.9434 - val_loss: 2.0856 - val_auc: 0.6701 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0216 - auc: 0.9750\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0216 - auc: 0.9750 - val_loss: 1.4829 - val_auc: 0.7062 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0159 - auc: 0.9878 - val_loss: 1.1545 - val_auc: 0.7097 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 12s 989ms/step - loss: 0.0138 - auc: 0.9935 - val_loss: 0.6877 - val_auc: 0.7079 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0122 - auc: 0.9969\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0122 - auc: 0.9969 - val_loss: 0.2696 - val_auc: 0.7117 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 11s 946ms/step - loss: 0.0112 - auc: 0.9984 - val_loss: 0.1034 - val_auc: 0.7246 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 11s 946ms/step - loss: 0.0111 - auc: 0.9986 - val_loss: 0.0582 - val_auc: 0.7281 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0109 - auc: 0.9987\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 11s 900ms/step - loss: 0.0109 - auc: 0.9987 - val_loss: 0.0575 - val_auc: 0.7084 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 11s 938ms/step - loss: 0.0108 - auc: 0.9988 - val_loss: 0.0605 - val_auc: 0.6861 - lr: 1.0000e-06\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 11s 905ms/step - loss: 0.0108 - auc: 0.9988 - val_loss: 0.0634 - val_auc: 0.6571 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0107 - auc: 0.9988\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "12/12 [==============================] - 11s 924ms/step - loss: 0.0107 - auc: 0.9988 - val_loss: 0.0663 - val_auc: 0.6310 - lr: 1.0000e-06\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0107 - auc: 0.9988Restoring model weights from the end of the best epoch: 12.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0107 - auc: 0.9988 - val_loss: 0.0688 - val_auc: 0.6136 - lr: 1.0000e-07\n",
      "Epoch 17: early stopping\n",
      "[                                                  ] 0.78%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x700d0b328790> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 35s 2s/step - loss: 0.3212 - auc: 0.6027 - val_loss: 0.2769 - val_auc: 0.4909 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 29s 2s/step - loss: 0.1865 - auc: 0.7630 - val_loss: 0.3400 - val_auc: 0.6989 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 34s 3s/step - loss: 0.1286 - auc: 0.8020 - val_loss: 1.4888 - val_auc: 0.6856 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0843 - auc: 0.8505\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 31s 3s/step - loss: 0.0843 - auc: 0.8505 - val_loss: 1.8974 - val_auc: 0.6623 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 31s 3s/step - loss: 0.0569 - auc: 0.8840 - val_loss: 1.2687 - val_auc: 0.6947 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 31s 3s/step - loss: 0.0560 - auc: 0.8884 - val_loss: 0.9008 - val_auc: 0.7064 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0590 - auc: 0.8972\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.0590 - auc: 0.8972 - val_loss: 0.7165 - val_auc: 0.7275 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.0445 - auc: 0.9222 - val_loss: 0.3825 - val_auc: 0.7616 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 11s 951ms/step - loss: 0.0535 - auc: 0.9086 - val_loss: 0.1771 - val_auc: 0.8060 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0454 - auc: 0.9296\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0454 - auc: 0.9296 - val_loss: 0.1028 - val_auc: 0.8325 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.0482 - auc: 0.9223 - val_loss: 0.0726 - val_auc: 0.8387 - lr: 1.0000e-06\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 0.0422 - auc: 0.9232 - val_loss: 0.0612 - val_auc: 0.8295 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0451 - auc: 0.9195\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "12/12 [==============================] - 36s 3s/step - loss: 0.0451 - auc: 0.9195 - val_loss: 0.0590 - val_auc: 0.8238 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 29s 2s/step - loss: 0.0515 - auc: 0.9199 - val_loss: 0.0601 - val_auc: 0.8171 - lr: 1.0000e-07\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.0454 - auc: 0.9264 - val_loss: 0.0615 - val_auc: 0.8147 - lr: 1.0000e-07\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0457 - auc: 0.9194Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.0457 - auc: 0.9194 - val_loss: 0.0624 - val_auc: 0.8119 - lr: 1.0000e-07\n",
      "Epoch 16: early stopping\n",
      "[                                                  ] 1.56%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: True - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x700d0b328820> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 24s 1s/step - loss: 0.9719 - auc: 0.8123 - val_loss: 1.0420 - val_auc: 0.6595 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 903ms/step - loss: 0.9237 - auc: 0.8558 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 906ms/step - loss: 0.8836 - auc: 0.8876 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 11s 919ms/step - loss: 0.8970 - auc: 0.8582 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 928ms/step - loss: 0.9276 - auc: 0.8138 - val_loss: 1.2198 - val_auc: 0.4295 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9795 - auc: 0.7443Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 11s 964ms/step - loss: 0.9795 - auc: 0.7443 - val_loss: 1.2233 - val_auc: 0.4273 - lr: 0.0010\n",
      "Epoch 6: early stopping\n",
      "[=                                                 ] 2.34%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: True - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x700d0b328820> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 23s 1s/step - loss: 1.0557 - auc: 0.6789 - val_loss: 1.1952 - val_auc: 0.3710 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 959ms/step - loss: 0.9610 - auc: 0.7997 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 12s 965ms/step - loss: 0.9196 - auc: 0.8532 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8950 - auc: 0.8677\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 11s 952ms/step - loss: 0.8950 - auc: 0.8677 - val_loss: 1.0597 - val_auc: 0.6068 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 937ms/step - loss: 0.8750 - auc: 0.8961 - val_loss: 1.0066 - val_auc: 0.8101 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 11s 902ms/step - loss: 0.8590 - auc: 0.9135 - val_loss: 0.9633 - val_auc: 0.8515 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8511 - auc: 0.9212\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 11s 934ms/step - loss: 0.8511 - auc: 0.9212 - val_loss: 1.0183 - val_auc: 0.7401 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 11s 896ms/step - loss: 0.8450 - auc: 0.9254 - val_loss: 1.0757 - val_auc: 0.6201 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 11s 905ms/step - loss: 0.8436 - auc: 0.9262 - val_loss: 1.1129 - val_auc: 0.5600 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8430 - auc: 0.9266\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 11s 916ms/step - loss: 0.8430 - auc: 0.9266 - val_loss: 1.1373 - val_auc: 0.5252 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8427 - auc: 0.9291Restoring model weights from the end of the best epoch: 6.\n",
      "12/12 [==============================] - 11s 892ms/step - loss: 0.8427 - auc: 0.9291 - val_loss: 1.1515 - val_auc: 0.4970 - lr: 1.0000e-06\n",
      "Epoch 11: early stopping\n",
      "[=                                                 ] 3.12%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: False - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x700d0b328790> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 19s 1s/step - loss: 0.1003 - auc: 0.6845 - val_loss: 0.0617 - val_auc: 0.7179 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 944ms/step - loss: 0.0504 - auc: 0.8052 - val_loss: 0.0558 - val_auc: 0.7792 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 956ms/step - loss: 0.0430 - auc: 0.8647 - val_loss: 0.0505 - val_auc: 0.7998 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0379 - auc: 0.9074\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 11s 930ms/step - loss: 0.0379 - auc: 0.9074 - val_loss: 0.0456 - val_auc: 0.8448 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 942ms/step - loss: 0.0339 - auc: 0.9375 - val_loss: 0.0464 - val_auc: 0.8424 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 11s 950ms/step - loss: 0.0332 - auc: 0.9382 - val_loss: 0.0454 - val_auc: 0.8475 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0327 - auc: 0.9392\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 11s 934ms/step - loss: 0.0327 - auc: 0.9392 - val_loss: 0.0452 - val_auc: 0.8480 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 12s 985ms/step - loss: 0.0321 - auc: 0.9460 - val_loss: 0.0450 - val_auc: 0.8501 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 11s 966ms/step - loss: 0.0321 - auc: 0.9460 - val_loss: 0.0451 - val_auc: 0.8484 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0320 - auc: 0.9454\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 12s 978ms/step - loss: 0.0320 - auc: 0.9454 - val_loss: 0.0452 - val_auc: 0.8477 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 11s 962ms/step - loss: 0.0320 - auc: 0.9455 - val_loss: 0.0452 - val_auc: 0.8479 - lr: 1.0000e-06\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 11s 965ms/step - loss: 0.0320 - auc: 0.9455 - val_loss: 0.0452 - val_auc: 0.8479 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0320 - auc: 0.9454Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "12/12 [==============================] - 11s 957ms/step - loss: 0.0320 - auc: 0.9454 - val_loss: 0.0452 - val_auc: 0.8480 - lr: 1.0000e-06\n",
      "Epoch 13: early stopping\n",
      "[=                                                 ] 3.91%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: False - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x700d0b328790> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 19s 1s/step - loss: 0.2491 - auc: 0.6173 - val_loss: 0.1272 - val_auc: 0.5826 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 944ms/step - loss: 0.1579 - auc: 0.7745 - val_loss: 0.0736 - val_auc: 0.7378 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 12s 979ms/step - loss: 0.1072 - auc: 0.8336 - val_loss: 0.1035 - val_auc: 0.7210 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0786 - auc: 0.8863\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 11s 942ms/step - loss: 0.0786 - auc: 0.8863 - val_loss: 0.0698 - val_auc: 0.8061 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 952ms/step - loss: 0.0649 - auc: 0.8999 - val_loss: 0.0641 - val_auc: 0.8172 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 11s 971ms/step - loss: 0.0608 - auc: 0.9131 - val_loss: 0.0686 - val_auc: 0.8037 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0572 - auc: 0.9220\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 11s 951ms/step - loss: 0.0572 - auc: 0.9220 - val_loss: 0.0708 - val_auc: 0.7999 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 12s 982ms/step - loss: 0.0532 - auc: 0.9273 - val_loss: 0.0697 - val_auc: 0.8059 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 11s 927ms/step - loss: 0.0524 - auc: 0.9229 - val_loss: 0.0693 - val_auc: 0.8114 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0547 - auc: 0.9230Restoring model weights from the end of the best epoch: 5.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 11s 936ms/step - loss: 0.0547 - auc: 0.9230 - val_loss: 0.0694 - val_auc: 0.8159 - lr: 1.0000e-05\n",
      "Epoch 10: early stopping\n",
      "[==                                                ] 4.69%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: False - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x700d0b328820> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 19s 1s/step - loss: 1.0209 - auc: 0.6974 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 950ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 969ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 11s 952ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 12s 976ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 11s 937ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-04\n",
      "Epoch 6: early stopping\n",
      "[==                                                ] 5.47%\u001B[92mModel: DenseNet121 - LR: 0.001 - Fine Tune: False - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x700d0b328820> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 19s 1s/step - loss: 1.0664 - auc: 0.6619 - val_loss: 1.0958 - val_auc: 0.5987 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 12s 988ms/step - loss: 0.9993 - auc: 0.7673 - val_loss: 1.0625 - val_auc: 0.6719 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 12s 986ms/step - loss: 0.9524 - auc: 0.8385 - val_loss: 1.0322 - val_auc: 0.7559 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9191 - auc: 0.8703\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9191 - auc: 0.8703 - val_loss: 1.0354 - val_auc: 0.7549 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 958ms/step - loss: 0.9043 - auc: 0.8919 - val_loss: 1.0278 - val_auc: 0.7704 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 12s 976ms/step - loss: 0.8970 - auc: 0.9004 - val_loss: 1.0231 - val_auc: 0.7823 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8950 - auc: 0.9030\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "12/12 [==============================] - 11s 963ms/step - loss: 0.8950 - auc: 0.9030 - val_loss: 1.0212 - val_auc: 0.7817 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 11s 944ms/step - loss: 0.8929 - auc: 0.9035 - val_loss: 1.0194 - val_auc: 0.7838 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 11s 934ms/step - loss: 0.8921 - auc: 0.9055 - val_loss: 1.0174 - val_auc: 0.7865 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8881 - auc: 0.9009\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "12/12 [==============================] - 11s 932ms/step - loss: 0.8881 - auc: 0.9009 - val_loss: 1.0153 - val_auc: 0.7896 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8861 - auc: 0.9071 - val_loss: 1.0133 - val_auc: 0.7907 - lr: 1.0000e-06\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 11s 971ms/step - loss: 0.8927 - auc: 0.8980 - val_loss: 1.0113 - val_auc: 0.7908 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8925 - auc: 0.9036\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "12/12 [==============================] - 11s 966ms/step - loss: 0.8925 - auc: 0.9036 - val_loss: 1.0094 - val_auc: 0.7933 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 11s 961ms/step - loss: 0.8888 - auc: 0.9068 - val_loss: 1.0075 - val_auc: 0.7930 - lr: 1.0000e-07\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 11s 950ms/step - loss: 0.8912 - auc: 0.9026 - val_loss: 1.0056 - val_auc: 0.7945 - lr: 1.0000e-07\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8922 - auc: 0.9062\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "12/12 [==============================] - 11s 930ms/step - loss: 0.8922 - auc: 0.9062 - val_loss: 1.0038 - val_auc: 0.7953 - lr: 1.0000e-07\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 11s 950ms/step - loss: 0.8904 - auc: 0.9033 - val_loss: 1.0021 - val_auc: 0.7958 - lr: 1.0000e-08\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 11s 927ms/step - loss: 0.8863 - auc: 0.9065 - val_loss: 1.0005 - val_auc: 0.7967 - lr: 1.0000e-08\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8901 - auc: 0.9063\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "12/12 [==============================] - 11s 959ms/step - loss: 0.8901 - auc: 0.9063 - val_loss: 0.9990 - val_auc: 0.7959 - lr: 1.0000e-08\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 12s 998ms/step - loss: 0.8885 - auc: 0.9052 - val_loss: 0.9976 - val_auc: 0.7972 - lr: 1.0000e-09\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 12s 994ms/step - loss: 0.8896 - auc: 0.9052 - val_loss: 0.9963 - val_auc: 0.7970 - lr: 1.0000e-09\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8883 - auc: 0.9064\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "12/12 [==============================] - 11s 976ms/step - loss: 0.8883 - auc: 0.9064 - val_loss: 0.9951 - val_auc: 0.7967 - lr: 1.0000e-09\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 11s 964ms/step - loss: 0.8873 - auc: 0.9057 - val_loss: 0.9940 - val_auc: 0.7962 - lr: 1.0000e-10\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 11s 954ms/step - loss: 0.8909 - auc: 0.9071 - val_loss: 0.9929 - val_auc: 0.7968 - lr: 1.0000e-10\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8918 - auc: 0.9018\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8918 - auc: 0.9018 - val_loss: 0.9920 - val_auc: 0.7977 - lr: 1.0000e-10\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 11s 945ms/step - loss: 0.8888 - auc: 0.9100 - val_loss: 0.9912 - val_auc: 0.7984 - lr: 1.0000e-11\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.8943 - auc: 0.9026 - val_loss: 0.9904 - val_auc: 0.7979 - lr: 1.0000e-11\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8891 - auc: 0.9034\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "12/12 [==============================] - 12s 987ms/step - loss: 0.8891 - auc: 0.9034 - val_loss: 0.9897 - val_auc: 0.7966 - lr: 1.0000e-11\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 12s 998ms/step - loss: 0.8874 - auc: 0.9093 - val_loss: 0.9891 - val_auc: 0.7972 - lr: 1.0000e-12\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 11s 944ms/step - loss: 0.8892 - auc: 0.9051 - val_loss: 0.9885 - val_auc: 0.7967 - lr: 1.0000e-12\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8916 - auc: 0.9035Restoring model weights from the end of the best epoch: 26.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "12/12 [==============================] - 11s 913ms/step - loss: 0.8916 - auc: 0.9035 - val_loss: 0.9880 - val_auc: 0.7969 - lr: 1.0000e-12\n",
      "Epoch 31: early stopping\n",
      "[===                                               ] 6.25%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x700d0b328790> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 23s 1s/step - loss: 1.9094 - auc: 0.6799 - val_loss: 4.1661 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 920ms/step - loss: 2.0873 - auc: 0.7057 - val_loss: 4.1661 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 921ms/step - loss: 2.0843 - auc: 0.7066 - val_loss: 4.1661 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1714 - auc: 0.6939\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 11s 951ms/step - loss: 2.1714 - auc: 0.6939 - val_loss: 4.1661 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 941ms/step - loss: 2.3761 - auc: 0.6656 - val_loss: 4.1661 - val_auc: 0.4136 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2634 - auc: 0.6812Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 12s 978ms/step - loss: 2.2634 - auc: 0.6812 - val_loss: 4.1661 - val_auc: 0.4136 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[===                                               ] 7.03%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: True - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x700d0b328790> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 25s 1s/step - loss: 0.2689 - auc: 0.5908 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0823 - auc: 0.7165 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0611 - auc: 0.7663 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0528 - auc: 0.7956\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0528 - auc: 0.7956 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0518 - auc: 0.8102 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0633 - auc: 0.7858Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 12s 986ms/step - loss: 0.0633 - auc: 0.7858 - val_loss: 2.4222 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[===                                               ] 7.81%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: True - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x700d0b328820> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 24s 1s/step - loss: 1.0100 - auc: 0.7115 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 955ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 945ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 11s 934ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 962ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 11s 960ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[====                                              ] 8.59%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: True - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x700d0b328820> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 25s 1s/step - loss: 1.0389 - auc: 0.6822 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 12s 1s/step - loss: 1.0200 - auc: 0.6887 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 946ms/step - loss: 1.0242 - auc: 0.6842 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0123 - auc: 0.6993\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 12s 964ms/step - loss: 1.0123 - auc: 0.6993 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 932ms/step - loss: 0.9963 - auc: 0.7190 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9939 - auc: 0.7221Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 11s 942ms/step - loss: 0.9939 - auc: 0.7221 - val_loss: 1.2340 - val_auc: 0.4136 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[====                                              ] 9.38%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: False - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x700d0b328790> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 19s 1s/step - loss: 3.6626 - auc: 0.4187 - val_loss: 4.0692 - val_auc: 0.4273 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 915ms/step - loss: 4.1755 - auc: 0.4123 - val_loss: 4.0692 - val_auc: 0.4273 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 948ms/step - loss: 4.1755 - auc: 0.4123 - val_loss: 4.0692 - val_auc: 0.4273 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 4.1755 - auc: 0.4123\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 11s 931ms/step - loss: 4.1755 - auc: 0.4123 - val_loss: 4.0692 - val_auc: 0.4273 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 945ms/step - loss: 4.1755 - auc: 0.4123 - val_loss: 4.0692 - val_auc: 0.4273 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 4.1755 - auc: 0.4123Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 4.1755 - auc: 0.4123 - val_loss: 4.0692 - val_auc: 0.4273 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[=====                                             ] 10.16%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: False - Loss: <function focal_loss.<locals>.focal_loss_fixed at 0x700d0b328790> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 20s 1s/step - loss: 0.4352 - auc: 0.6545 - val_loss: 0.2854 - val_auc: 0.7072 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 932ms/step - loss: 0.1429 - auc: 0.7805 - val_loss: 0.2521 - val_auc: 0.7612 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 950ms/step - loss: 0.1329 - auc: 0.7936 - val_loss: 0.2517 - val_auc: 0.6587 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 11s 925ms/step - loss: 0.1202 - auc: 0.8104 - val_loss: 0.2955 - val_auc: 0.6616 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 919ms/step - loss: 0.1349 - auc: 0.8373 - val_loss: 0.2290 - val_auc: 0.7570 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1304 - auc: 0.8410\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 11s 940ms/step - loss: 0.1304 - auc: 0.8410 - val_loss: 0.3129 - val_auc: 0.7787 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 11s 925ms/step - loss: 0.1033 - auc: 0.8683 - val_loss: 0.1961 - val_auc: 0.8064 - lr: 1.0000e-03\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 11s 939ms/step - loss: 0.0707 - auc: 0.9013 - val_loss: 0.0968 - val_auc: 0.8440 - lr: 1.0000e-03\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0582 - auc: 0.9150\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0582 - auc: 0.9150 - val_loss: 0.0881 - val_auc: 0.8427 - lr: 1.0000e-03\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 11s 968ms/step - loss: 0.0528 - auc: 0.9188 - val_loss: 0.0851 - val_auc: 0.8452 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 11s 950ms/step - loss: 0.0472 - auc: 0.9230 - val_loss: 0.0837 - val_auc: 0.8463 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0545 - auc: 0.9160\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "12/12 [==============================] - 11s 915ms/step - loss: 0.0545 - auc: 0.9160 - val_loss: 0.0819 - val_auc: 0.8471 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 11s 906ms/step - loss: 0.0536 - auc: 0.9243 - val_loss: 0.0806 - val_auc: 0.8470 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 11s 918ms/step - loss: 0.0517 - auc: 0.9221 - val_loss: 0.0796 - val_auc: 0.8469 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0560 - auc: 0.9097\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0560 - auc: 0.9097 - val_loss: 0.0789 - val_auc: 0.8470 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 11s 925ms/step - loss: 0.0554 - auc: 0.9159 - val_loss: 0.0783 - val_auc: 0.8465 - lr: 1.0000e-06\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0579 - auc: 0.9163Restoring model weights from the end of the best epoch: 12.\n",
      "12/12 [==============================] - 12s 991ms/step - loss: 0.0579 - auc: 0.9163 - val_loss: 0.0777 - val_auc: 0.8467 - lr: 1.0000e-06\n",
      "Epoch 17: early stopping\n",
      "[=====                                             ] 10.94%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: False - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x700d0b328820> - Top: CAM\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 21s 1s/step - loss: 1.0208 - auc: 0.6969 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 975ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 918ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 11s 944ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 11s 976ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - auc: 0.7008Restoring model weights from the end of the best epoch: 1.\n",
      "12/12 [==============================] - 12s 996ms/step - loss: 1.0094 - auc: 0.7008 - val_loss: 1.0420 - val_auc: 0.6591 - lr: 1.0000e-03\n",
      "Epoch 6: early stopping\n",
      "[=====                                             ] 11.72%\u001B[92mModel: DenseNet121 - LR: 0.01 - Fine Tune: False - Loss: <function w_cel_loss.<locals>.weighted_cross_entropy_with_logits at 0x700d0b328820> - Top: Standard\u001B[0m\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 20s 1s/step - loss: 1.0807 - auc: 0.6281 - val_loss: 1.2105 - val_auc: 0.4433 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 11s 924ms/step - loss: 1.0267 - auc: 0.6957 - val_loss: 1.2058 - val_auc: 0.4418 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 11s 965ms/step - loss: 1.0074 - auc: 0.7175 - val_loss: 1.2059 - val_auc: 0.4449 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 11s 956ms/step - loss: 0.9970 - auc: 0.7298 - val_loss: 1.1459 - val_auc: 0.5319 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9853 - auc: 0.7424\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 11s 939ms/step - loss: 0.9853 - auc: 0.7424 - val_loss: 1.2096 - val_auc: 0.4536 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 11s 934ms/step - loss: 0.9735 - auc: 0.7606 - val_loss: 1.1768 - val_auc: 0.4949 - lr: 1.0000e-03\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 11s 965ms/step - loss: 0.9683 - auc: 0.7647 - val_loss: 1.1364 - val_auc: 0.5673 - lr: 1.0000e-03\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9641 - auc: 0.7741\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "12/12 [==============================] - 11s 958ms/step - loss: 0.9641 - auc: 0.7741 - val_loss: 1.0960 - val_auc: 0.6129 - lr: 1.0000e-03\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 11s 942ms/step - loss: 0.9597 - auc: 0.7769 - val_loss: 1.0598 - val_auc: 0.6501 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 11s 926ms/step - loss: 0.9695 - auc: 0.7700 - val_loss: 1.0347 - val_auc: 0.6718 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9610 - auc: 0.7718\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "12/12 [==============================] - 12s 982ms/step - loss: 0.9610 - auc: 0.7718 - val_loss: 1.0225 - val_auc: 0.6939 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 12s 998ms/step - loss: 0.9624 - auc: 0.7803 - val_loss: 1.0269 - val_auc: 0.7140 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 11s 928ms/step - loss: 0.9643 - auc: 0.7780 - val_loss: 1.0302 - val_auc: 0.7139 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9660 - auc: 0.7733\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.9660 - auc: 0.7733 - val_loss: 1.0289 - val_auc: 0.7137 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9634 - auc: 0.7755 - val_loss: 1.0272 - val_auc: 0.7161 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 12s 988ms/step - loss: 0.9634 - auc: 0.7819 - val_loss: 1.0271 - val_auc: 0.7107 - lr: 1.0000e-06\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9576 - auc: 0.7808\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "12/12 [==============================] - 12s 983ms/step - loss: 0.9576 - auc: 0.7808 - val_loss: 1.0271 - val_auc: 0.7087 - lr: 1.0000e-06\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 11s 963ms/step - loss: 0.9636 - auc: 0.7746 - val_loss: 1.0265 - val_auc: 0.7095 - lr: 1.0000e-07\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 12s 979ms/step - loss: 0.9616 - auc: 0.7739 - val_loss: 1.0262 - val_auc: 0.7060 - lr: 1.0000e-07\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9601 - auc: 0.7777Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.9601 - auc: 0.7777 - val_loss: 1.0264 - val_auc: 0.7068 - lr: 1.0000e-07\n",
      "Epoch 20: early stopping\n",
      "[======                                            ] 12.50%"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type float32 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m                     bar\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../cell_division/results/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbase_model\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.json\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 45\u001B[0m         \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m     results \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     48\u001B[0m bar\u001B[38;5;241m.\u001B[39mend()\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/json/__init__.py:179\u001B[0m, in \u001B[0;36mdump\u001B[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n\u001B[1;32m    173\u001B[0m     iterable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(skipkeys\u001B[38;5;241m=\u001B[39mskipkeys, ensure_ascii\u001B[38;5;241m=\u001B[39mensure_ascii,\n\u001B[1;32m    174\u001B[0m         check_circular\u001B[38;5;241m=\u001B[39mcheck_circular, allow_nan\u001B[38;5;241m=\u001B[39mallow_nan, indent\u001B[38;5;241m=\u001B[39mindent,\n\u001B[1;32m    175\u001B[0m         separators\u001B[38;5;241m=\u001B[39mseparators,\n\u001B[1;32m    176\u001B[0m         default\u001B[38;5;241m=\u001B[39mdefault, sort_keys\u001B[38;5;241m=\u001B[39msort_keys, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\u001B[38;5;241m.\u001B[39miterencode(obj)\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;66;03m# a debuggability cost\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m    180\u001B[0m     fp\u001B[38;5;241m.\u001B[39mwrite(chunk)\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/json/encoder.py:431\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode\u001B[0;34m(o, _current_indent_level)\u001B[0m\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _iterencode_list(o, _current_indent_level)\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(o, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 431\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _iterencode_dict(o, _current_indent_level)\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m markers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/json/encoder.py:405\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[0;34m(dct, _current_indent_level)\u001B[0m\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    404\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 405\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    407\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/json/encoder.py:405\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[0;34m(dct, _current_indent_level)\u001B[0m\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    404\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 405\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    407\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/json/encoder.py:325\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_list\u001B[0;34m(lst, _current_indent_level)\u001B[0m\n\u001B[1;32m    323\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    324\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 325\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    327\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/json/encoder.py:438\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode\u001B[0;34m(o, _current_indent_level)\u001B[0m\n\u001B[1;32m    436\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCircular reference detected\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    437\u001B[0m     markers[markerid] \u001B[38;5;241m=\u001B[39m o\n\u001B[0;32m--> 438\u001B[0m o \u001B[38;5;241m=\u001B[39m \u001B[43m_default\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m _iterencode(o, _current_indent_level)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m markers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/mambaforge/envs/py310ml/lib/python3.10/json/encoder.py:179\u001B[0m, in \u001B[0;36mJSONEncoder.default\u001B[0;34m(self, o)\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault\u001B[39m(\u001B[38;5;28mself\u001B[39m, o):\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03m    (to raise a ``TypeError``).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    177\u001B[0m \n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    180\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis not JSON serializable\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Object of type float32 is not JSON serializable"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T15:22:47.617104Z",
     "start_time": "2024-08-02T15:22:47.616977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(json.dumps(results, indent=4))\n",
    "# Save\n",
    "with open('../cell_division/results/grid_search_cnn.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ],
   "id": "87db1c7a4b035978",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
